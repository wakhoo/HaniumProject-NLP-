{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import re, nltk, spacy, gensim\n",
    "\n",
    "from sklearn.decomposition import LatentDirichletAllocation, TruncatedSVD\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from pprint import pprint\n",
    "\n",
    "import pyLDAvis\n",
    "import pyLDAvis.sklearn\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "%matplotlib inline\n",
    "df = pd.read_csv(r'train_clean2.csv', error_bad_lines=False,encoding=\"cp949\")\n",
    "data = df[['abstract']]\n",
    "data['abstract'] = data.apply(lambda row: nltk.word_tokenize(row['abstract']), axis=1) #문제해결을 위한 토큰화\n",
    "\n",
    "\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "data['abstract'] = data['abstract'].apply(lambda x: [WordNetLemmatizer().lemmatize(word, pos='v') for word in x])\n",
    "\n",
    "tokenized_doc = data['abstract'].apply(lambda x: [word for word in x if len(word) > 3])\n",
    "\n",
    "\n",
    "\n",
    "# TODO 토큰화 되돌리기\n",
    "detokenized_doc = []\n",
    "for i in range(len(data)):\n",
    "    t = ' '.join(tokenized_doc[i])\n",
    "    detokenized_doc.append(t)\n",
    "\n",
    "data['abstract'] = detokenized_doc\n",
    "\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "vectorizer = TfidfVectorizer(stop_words='english') # 상위 1,000개의 단어를 보존\n",
    "#vectorizer = TfidfVectorizer(stop_words='english') 아마 max_features없이 그냥 stop_words만 매개변수로 넣어줘도 될 듯\n",
    "#a = vectorizer.fit_transform(text['abstract']).toarray()\n",
    "#print(a.shape) 총 단어 개수 확인\n",
    "\n",
    "\n",
    "data_vectorized = vectorizer.fit_transform(data['abstract'])\n",
    "\n",
    "from sklearn.decomposition import LatentDirichletAllocation\n",
    "lda_model=LatentDirichletAllocation(n_components=15,learning_method='online',random_state=777,max_iter=1)\n",
    "\n",
    "\n",
    "lda_top=lda_model.fit_transform(data_vectorized)\n",
    "\n",
    "lda_output = lda_model.transform(data_vectorized)\n",
    "topicnames = [\"Topic\" + str(i) for i in range(lda_model.n_components)]\n",
    "docnames = [\"Doc\" + str(i) for i in range(len(data))]\n",
    "df_document_topic = pd.DataFrame(np.round(lda_output, 2), columns=topicnames, index=docnames)\n",
    "dominant_topic = np.argmax(df_document_topic.values, axis=1)\n",
    "df_document_topic['dominant_topic'] = dominant_topic\n",
    "\n",
    "def color_green(val):\n",
    " color = 'green' if val > .1 else 'black'\n",
    " return 'color: {col}'.format(col=color)\n",
    "def make_bold(val):\n",
    " weight = 700 if val > .1 else 400\n",
    " return 'font-weight: {weight}'.format(weight=weight)\n",
    "df_document_topics = df_document_topic.style.applymap(color_green).applymap(make_bold)\n",
    "df_document_topics\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  },
  "pycharm": {
   "stem_cell": {
    "cell_type": "raw",
    "source": [],
    "metadata": {
     "collapsed": false
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}